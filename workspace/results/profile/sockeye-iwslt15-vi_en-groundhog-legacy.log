[INFO:sockeye.utils] Sockeye version 1.12.3 commit unknown
[INFO:sockeye.utils] MXNet version 0.12.1
[INFO:sockeye.utils] Command: /home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/sockeye/train.py --source /home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/train-preproc.vi --target /home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/train-preproc.en --source-vocab /home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/vocab.vi --target-vocab /home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/vocab.en --validation-source /home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/tst2012.vi --validation-target /home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/tst2012.en --output /home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/iwslt15-vi_en --seed=1 --batch-type=sentence --batch-size=80 --bucket-width=10 --checkpoint-frequency=2000 --device-ids=0 --embed-dropout=0.3:0.3 --encoder=rnn --decoder=rnn --num-layers=1:1 --rnn-cell-type=lstm --rnn-num-hidden=1000 --rnn-residual-connections --layer-normalization --rnn-attention-type=mlp --rnn-attention-num-hidden=512 --rnn-attention-use-prev-word --rnn-attention-in-upper-layers --rnn-attention-coverage-num-hidden=1 --rnn-attention-coverage-type=count --rnn-decoder-state-init=zero --rnn-attention-use-prev-word --rnn-dropout-inputs=0:0 --rnn-dropout-states=0.0:0.0 --rnn-dropout-recurrent=0.0:0.0 --rnn-decoder-hidden-dropout=0.3 --fill-up=replicate --max-seq-len=50:50 --loss=cross-entropy --num-embed 500:500 --num-words 50000:50000 --word-min-count 1:1 --optimizer=adam --optimized-metric=perplexity --clip-gradient=1.0 --initial-learning-rate=0.0002 --learning-rate-reduce-num-not-improved=8 --learning-rate-reduce-factor=0.7 --learning-rate-scheduler-type=plateau-reduce --learning-rate-warmup=0 --max-num-checkpoint-not-improved=16 --min-num-epochs=1 --monitor-bleu=500 --keep-last-params=60 --lock-dir /var/lock --use-tensorboard --max-updates=500
[INFO:sockeye.utils] Arguments: Namespace(batch_size=80, batch_type='sentence', bucket_width=10, checkpoint_frequency=2000, clip_gradient=1.0, cnn_activation_type='glu', cnn_hidden_dropout=0.0, cnn_kernel_width=(3, 5), cnn_num_hidden=512, cnn_positional_embedding_type='learned', cnn_project_qkv=False, conv_embed_add_positional_encodings=False, conv_embed_dropout=0.0, conv_embed_max_filter_width=8, conv_embed_num_filters=(200, 200, 250, 250, 300, 300, 300, 300), conv_embed_num_highway_layers=4, conv_embed_output_dim=None, conv_embed_pool_stride=5, decoder='rnn', device_ids=[0], disable_device_locking=False, embed_dropout=(0.3, 0.3), embed_weight_init='default', encoder='rnn', fill_up='replicate', initial_learning_rate=0.0002, keep_last_params=60, kvstore='device', label_smoothing=0.0, layer_normalization=True, learn_lexical_bias=False, learning_rate_decay_optimizer_states_reset='off', learning_rate_decay_param_reset=False, learning_rate_half_life=10, learning_rate_reduce_factor=0.7, learning_rate_reduce_num_not_improved=8, learning_rate_schedule=None, learning_rate_scheduler_type='plateau-reduce', learning_rate_warmup=0, lexical_bias=None, limit=None, lock_dir='/var/lock', loss='cross-entropy', loss_normalization_type='valid', max_num_checkpoint_not_improved=16, max_num_epochs=None, max_seq_len=(50, 50), max_updates=500, metrics=['perplexity'], min_num_epochs=1, momentum=None, monitor_bleu=500, monitor_pattern=None, monitor_stat_func='mx_default', no_bucketing=False, num_embed=(500, 500), num_layers=(1, 1), num_words=(50000, 50000), optimized_metric='perplexity', optimizer='adam', optimizer_params=None, output='/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/iwslt15-vi_en', overwrite_output=False, params=None, quiet=False, rnn_attention_coverage_num_hidden=1, rnn_attention_coverage_type='count', rnn_attention_in_upper_layers=True, rnn_attention_mhdot_heads=None, rnn_attention_num_hidden=512, rnn_attention_partial_fw_prop=False, rnn_attention_type='mlp', rnn_attention_use_prev_word=True, rnn_cell_type='lstm', rnn_context_gating=False, rnn_decoder_hidden_dropout=0.3, rnn_decoder_state_init='zero', rnn_dropout_inputs=(0.0, 0.0), rnn_dropout_recurrent=(0.0, 0.0), rnn_dropout_states=(0.0, 0.0), rnn_encoder_reverse_input=False, rnn_first_residual_layer=2, rnn_forget_bias=0.0, rnn_h2h_init='orthogonal', rnn_num_hidden=1000, rnn_residual_connections=True, seed=1, source='/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/train-preproc.vi', source_vocab='/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/vocab.vi', target='/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/train-preproc.en', target_vocab='/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/vocab.en', transformer_attention_heads=8, transformer_dropout_attention=0.0, transformer_dropout_prepost=0.0, transformer_dropout_relu=0.0, transformer_feed_forward_num_hidden=2048, transformer_model_size=512, transformer_positional_embedding_type='fixed', transformer_postprocess=('drn', 'drn'), transformer_preprocess=('', ''), use_cpu=False, use_fused_rnn=False, use_tensorboard=True, validation_source='/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/tst2012.vi', validation_target='/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/tst2012.en', weight_decay=0.0, weight_init='xavier', weight_init_scale=2.34, weight_init_xavier_factor_type='in', weight_normalization=False, weight_tying=False, weight_tying_type='trg_softmax', word_min_count=(1, 1))
[INFO:sockeye.utils] Attempting to acquire 1 GPUs of 1 GPUs. The requested devices are: [0]
[INFO:sockeye.utils] Acquired GPU 0.
[INFO:__main__] Device(s): GPU [0]
[INFO:sockeye.vocab] Building vocabulary from dataset(s): ['/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/vocab.vi']
[INFO:sockeye.vocab] Initial vocabulary: 7706 types
[INFO:sockeye.vocab] Pruned vocabulary: 7706 types (min frequency 1)
[INFO:sockeye.vocab] Final vocabulary: 7710 types (min frequency 1, top 50000 types)
[INFO:sockeye.vocab] Building vocabulary from dataset(s): ['/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/vocab.en']
[INFO:sockeye.vocab] Initial vocabulary: 17188 types
[INFO:sockeye.vocab] Pruned vocabulary: 17188 types (min frequency 1)
[INFO:sockeye.vocab] Final vocabulary: 17192 types (min frequency 1, top 50000 types)
[INFO:sockeye.vocab] Vocabulary saved to "/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/iwslt15-vi_en/vocab.src.json"
[INFO:sockeye.vocab] Vocabulary saved to "/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/iwslt15-vi_en/vocab.trg.json"
[INFO:__main__] Vocabulary sizes: source=7710 target=17192
[INFO:sockeye.data_io] Creating train data iterator
[INFO:sockeye.data_io] 133166 source sentences in '/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/train-preproc.vi'
[INFO:sockeye.data_io] 133166 target sentences in '/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/train-preproc.en'
[INFO:sockeye.data_io] Mean training target/source length ratio: 0.91 (+-0.21)
[INFO:sockeye.data_io] Source words: 2548656
[INFO:sockeye.data_io] Target words: 2229748
[INFO:sockeye.data_io] Vocab coverage source: 99%
[INFO:sockeye.data_io] Vocab coverage target: 98%
[INFO:sockeye.data_io] Total: 122154 samples in 6 buckets
[INFO:sockeye.data_io] Bucket of (10, 9) : 16949 samples in 212 batches of 80, approx 565.0 words/batch
[INFO:sockeye.data_io] Bucket of (20, 18) : 47291 samples in 592 batches of 80, approx 1024.0 words/batch
[INFO:sockeye.data_io] Bucket of (30, 27) : 30144 samples in 377 batches of 80, approx 1657.0 words/batch
[INFO:sockeye.data_io] Bucket of (40, 36) : 17519 samples in 219 batches of 80, approx 2283.7 words/batch
[INFO:sockeye.data_io] Bucket of (50, 45) : 9581 samples in 120 batches of 80, approx 2909.8 words/batch
[INFO:sockeye.data_io] Bucket of (50, 50) : 670 samples in 9 batches of 80, approx 3794.5 words/batch
[INFO:sockeye.data_io] 11012 sentence pairs out of buckets
[INFO:sockeye.data_io] fill up mode: replicate
[INFO:sockeye.data_io] 
[INFO:sockeye.data_io] Replicating 11 random sentences from bucket (10, 9) to size it to multiple of 80
[INFO:sockeye.data_io] Replicating 69 random sentences from bucket (20, 18) to size it to multiple of 80
[INFO:sockeye.data_io] Replicating 16 random sentences from bucket (30, 27) to size it to multiple of 80
[INFO:sockeye.data_io] Replicating 1 random sentences from bucket (40, 36) to size it to multiple of 80
[INFO:sockeye.data_io] Replicating 19 random sentences from bucket (50, 45) to size it to multiple of 80
[INFO:sockeye.data_io] Replicating 50 random sentences from bucket (50, 50) to size it to multiple of 80
[INFO:sockeye.data_io] Creating validation data iterator
[INFO:sockeye.data_io] Source words: 30139
[INFO:sockeye.data_io] Target words: 26226
[INFO:sockeye.data_io] Vocab coverage source: 99%
[INFO:sockeye.data_io] Vocab coverage target: 97%
[INFO:sockeye.data_io] Total: 1489 samples in 6 buckets
[INFO:sockeye.data_io] Bucket of (10, 9) : 223 samples in 3 batches of 80, approx 570.4 words/batch
[INFO:sockeye.data_io] Bucket of (20, 18) : 584 samples in 8 batches of 80, approx 1027.0 words/batch
[INFO:sockeye.data_io] Bucket of (30, 27) : 376 samples in 5 batches of 80, approx 1637.0 words/batch
[INFO:sockeye.data_io] Bucket of (40, 36) : 210 samples in 3 batches of 80, approx 2266.7 words/batch
[INFO:sockeye.data_io] Bucket of (50, 45) : 89 samples in 2 batches of 80, approx 2838.7 words/batch
[INFO:sockeye.data_io] Bucket of (50, 50) : 7 samples in 1 batches of 80, approx 3851.4 words/batch
[INFO:sockeye.data_io] 64 sentence pairs out of buckets
[INFO:sockeye.data_io] fill up mode: replicate
[INFO:sockeye.data_io] 
[INFO:sockeye.data_io] Replicating 17 random sentences from bucket (10, 9) to size it to multiple of 80
[INFO:sockeye.data_io] Replicating 56 random sentences from bucket (20, 18) to size it to multiple of 80
[INFO:sockeye.data_io] Replicating 24 random sentences from bucket (30, 27) to size it to multiple of 80
[INFO:sockeye.data_io] Replicating 30 random sentences from bucket (40, 36) to size it to multiple of 80
[INFO:sockeye.data_io] Replicating 71 random sentences from bucket (50, 45) to size it to multiple of 80
[INFO:sockeye.data_io] Replicating 73 random sentences from bucket (50, 50) to size it to multiple of 80
[INFO:sockeye.data_io] 1553 validation source sentences in '/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/train-preproc.vi'
[INFO:sockeye.data_io] 1553 validation target sentences in '/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/train-preproc.en'
[INFO:sockeye.lr_scheduler] Will reduce the learning rate by a factor of 0.70 whenever the validation score doesn't improve 8 times.
[INFO:sockeye.model] Config[_frozen=True, config_data=Config[_frozen=True, length_ratio_mean=0.9064821009611835, length_ratio_std=0.20957507036300008, max_observed_source_seq_len=50, max_observed_target_seq_len=50, source=/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/train-preproc.vi, target=/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/train-preproc.en, validation_source=/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/tst2012.vi, validation_target=/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/tst2012.en, vocab_source=/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/vocab.vi, vocab_target=/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/data/iwslt15-vi_en/vocab.en], config_decoder=Config[_frozen=True, attention_config=Config[_frozen=True, config_coverage=None, input_previous_word=True, layer_normalization=True, num_heads=None, num_hidden=512, partial_fw_prop=False, query_num_hidden=1000, source_num_hidden=1000, type=mlp], attention_in_upper_layers=True, context_gating=False, embed_dropout=0.3, hidden_dropout=0.3, layer_normalization=True, max_seq_len_source=50, num_embed=500, rnn_config=Config[_frozen=True, attention_in_upper_layers=False, cell_type=lstm, dropout_inputs=0.0, dropout_recurrent=0.0, dropout_states=0.0, first_residual_layer=2, forget_bias=0.0, num_hidden=1000, num_layers=1, residual=True], state_init=zero, vocab_size=17192, weight_normalization=False, weight_tying=False], config_encoder=Config[_frozen=True, conv_config=None, embed_dropout=0.3, num_embed=500, reverse_input=False, rnn_config=Config[_frozen=True, attention_in_upper_layers=False, cell_type=lstm, dropout_inputs=0.0, dropout_recurrent=0.0, dropout_states=0.0, first_residual_layer=2, forget_bias=0.0, num_hidden=1000, num_layers=1, residual=True], vocab_size=7710], config_loss=Config[_frozen=True, label_smoothing=0.0, name=cross-entropy, normalization_type=valid, vocab_size=17192], learn_lexical_bias=False, lexical_bias=None, max_seq_len_source=50, max_seq_len_target=50, vocab_source_size=7710, vocab_target_size=17192, weight_tying=False, weight_tying_type=None]
[INFO:sockeye.loss] Loss: CrossEntropy(normalization_type=valid, label_smoothing=0.0)
[INFO:sockeye.training] Using bucketing. Default max_seq_len=(50, 50)
[INFO:__main__] Optimizer: adam
[INFO:__main__] Optimizer Parameters: {'wd': 0.0, 'rescale_grad': 1.0, 'lr_scheduler': LearningRateSchedulerPlateauReduce(reduce_factor=0.70, reduce_num_not_improved=0), 'clip_gradient': 1.0, 'learning_rate': 0.0002}
[INFO:__main__] kvstore: device
[INFO:sockeye.model] Saved config to "/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/iwslt15-vi_en/config"
[INFO:sockeye.training] Model parameters: att_e2h_weight: (512, 1000), att_h2s_weight: (1, 512), att_norm_beta: (512,), att_norm_gamma: (512,), att_q2h_weight: (512, 1500), decoder_rnn_cls_bias: (17192,), decoder_rnn_cls_weight: (17192, 1000), decoder_rnn_hidden_bias: (1000,), decoder_rnn_hidden_norm_beta: (1000,), decoder_rnn_hidden_norm_gamma: (1000,), decoder_rnn_hidden_weight: (1000, 2000), decoder_rnn_l0_h2h_bias: (4000,), decoder_rnn_l0_h2h_weight: (4000, 1000), decoder_rnn_l0_i2h_bias: (4000,), decoder_rnn_l0_i2h_weight: (4000, 1500), encoder_birnn_forward_l0_h2h_bias: (2000,), encoder_birnn_forward_l0_h2h_weight: (2000, 500), encoder_birnn_forward_l0_i2h_bias: (2000,), encoder_birnn_forward_l0_i2h_weight: (2000, 500), encoder_birnn_reverse_l0_h2h_bias: (2000,), encoder_birnn_reverse_l0_h2h_weight: (2000, 500), encoder_birnn_reverse_l0_i2h_bias: (2000,), encoder_birnn_reverse_l0_i2h_weight: (2000, 500), source_embed_weight: (7710, 500), target_embed_weight: (17192, 500)
[INFO:sockeye.training] Total # of parameters: 46960728
[INFO:sockeye.checkpoint_decoder] Created CheckpointDecoder(max_input_len=-1, beam_size=5, model=/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/iwslt15-vi_en, num_sentences=500)
[INFO:sockeye.training] Training started.
[WARNING:root] The currently installed MXNet version 0.12.1 is less than 1.2.0. Some functionality of MXBoard may not work.
[WARNING:root] The currently installed MXNet version 0.12.1 is less than 1.2.0. Some functionality of MXBoard may not work.
[INFO:sockeye.callback] Logging training events for Tensorboard at '/home/armageddonknight/Workspace/EcoRNN/EcoNMT/benchmarks/sockeye/workspace/iwslt15-vi_en/tensorboard'
[INFO:sockeye.callback] Early stopping by optimizing 'perplexity'
[INFO:root] Epoch[0] Batch [50]	Speed: 357.91 samples/sec	perplexity=1013.276842
[INFO:numba.cuda.cudadrv.driver] init
[INFO:root] Epoch[0] Batch [100]	Speed: 378.63 samples/sec	perplexity=651.821585
[INFO:root] Epoch[0] Batch [150]	Speed: 363.98 samples/sec	perplexity=540.711956
[INFO:root] Epoch[0] Batch [200]	Speed: 361.05 samples/sec	perplexity=476.945279
[INFO:root] Epoch[0] Batch [250]	Speed: 388.82 samples/sec	perplexity=428.534630
[INFO:root] Epoch[0] Batch [300]	Speed: 354.92 samples/sec	perplexity=391.060267
[INFO:root] Epoch[0] Batch [350]	Speed: 366.50 samples/sec	perplexity=358.107779
[INFO:root] Epoch[0] Batch [400]	Speed: 318.68 samples/sec	perplexity=327.025657
[INFO:root] Epoch[0] Batch [450]	Speed: 352.79 samples/sec	perplexity=299.995928
[INFO:sockeye.training] Maximum # of updates (500) or epochs (None) reached.
[INFO:sockeye.training] Training stopped
[INFO:sockeye.training] Training finished. Best checkpoint: 0. Best validation perplexity: inf
[INFO:sockeye.utils] Releasing GPU 0.
